{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    precision_recall_fscore_support,\n",
    "    roc_curve, \n",
    "    auc\n",
    ")\n",
    "import pylidc as pl\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "# Optional: Use Albumentations for advanced augmentation\n",
    "USE_ALBUMENTATIONS = True\n",
    "if USE_ALBUMENTATIONS:\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# -----------------------------\n",
    "# Data Loading and Preprocessing\n",
    "# -----------------------------\n",
    "def load_nodule_data(\n",
    "    max_scans=400,\n",
    "    print_counts=True, \n",
    "    min_nodule_size=10, \n",
    "    max_nodule_size=50\n",
    "):\n",
    "    scans = pl.query(pl.Scan).all()\n",
    "    nodule_images, labels = [], []\n",
    "    processed_scan_ids = set()\n",
    "    stats = {\n",
    "        'total_scans': 0,\n",
    "        'total_nodules': 0,\n",
    "        'benign_nodules': 0,\n",
    "        'malignant_nodules': 0,\n",
    "        'discarded_nodules': 0\n",
    "    }\n",
    "\n",
    "    for scan_idx, scan in enumerate(scans[:max_scans]):\n",
    "        if scan.id in processed_scan_ids:\n",
    "            continue\n",
    "        stats['total_scans'] += 1\n",
    "        volume = scan.to_volume()\n",
    "        for nodule in scan.cluster_annotations():\n",
    "            stats['total_nodules'] += 1\n",
    "            try:\n",
    "                malignancy_scores = [anno.malignancy for anno in nodule if anno.malignancy is not None]\n",
    "                if not malignancy_scores:\n",
    "                    stats['discarded_nodules'] += 1\n",
    "                    continue\n",
    "                malignancy = np.mean(malignancy_scores)\n",
    "                label = 1 if malignancy >= 3 else 0\n",
    "                coords = np.mean([anno.centroid for anno in nodule], axis=0)\n",
    "                if np.any(np.isnan(coords)):\n",
    "                    stats['discarded_nodules'] += 1\n",
    "                    continue\n",
    "                x, y, z = np.round(coords).astype(int)\n",
    "                size = np.random.randint(min_nodule_size, max_nodule_size)\n",
    "                x_start = max(0, x - size // 2)\n",
    "                x_end = min(volume.shape[0], x + size // 2)\n",
    "                y_start = max(0, y - size // 2)\n",
    "                y_end = min(volume.shape[1], y + size // 2)\n",
    "                z_start = max(0, z - size // 2)\n",
    "                z_end = min(volume.shape[2], z + size // 2)\n",
    "                nodule_patch = volume[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "                if len(nodule_patch.shape) < 3:\n",
    "                    stats['discarded_nodules'] += 1\n",
    "                    continue\n",
    "                central_slice = nodule_patch[:, :, nodule_patch.shape[2] // 2]\n",
    "                resized_slice = resize(central_slice, (32, 32), mode='constant', anti_aliasing=True)\n",
    "                # Normalize robustly (results in floats, not 0-255)\n",
    "                resized_slice = (resized_slice - np.mean(resized_slice)) / (np.std(resized_slice) + 1e-8)\n",
    "                nodule_images.append(resized_slice)\n",
    "                labels.append(label)\n",
    "                if label == 1:\n",
    "                    stats['malignant_nodules'] += 1\n",
    "                else:\n",
    "                    stats['benign_nodules'] += 1\n",
    "            except Exception as e:\n",
    "                stats['discarded_nodules'] += 1\n",
    "\n",
    "    nodule_images = np.array(nodule_images)\n",
    "    labels = np.array(labels)\n",
    "    if print_counts:\n",
    "        print(\"\\nNodule Processing Statistics:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "    return nodule_images, labels\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset with Enhanced Data Augmentation\n",
    "# -----------------------------\n",
    "class LungNoduleDataset(Dataset):\n",
    "    def __init__(self, images, labels, mode='train'):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.mode = mode\n",
    "        if USE_ALBUMENTATIONS:\n",
    "            if self.mode == 'train':\n",
    "                self.transform = A.Compose([\n",
    "                    A.RandomRotate90(p=0.5),\n",
    "                    A.HorizontalFlip(p=0.5),\n",
    "                    A.Transpose(p=0.5),\n",
    "                    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "                    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "                    A.RandomBrightnessContrast(p=0.3),\n",
    "                    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.2),\n",
    "                    A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "                    ToTensorV2()\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = A.Compose([\n",
    "                    A.Resize(32, 32),\n",
    "                    A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "                    ToTensorV2()\n",
    "                ])\n",
    "        else:\n",
    "            if self.mode == 'train':\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.RandomRotation(degrees=15),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomVerticalFlip(p=0.5),\n",
    "                    transforms.RandomAffine(degrees=10, shear=5, scale=(0.9, 1.1)),\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.2),\n",
    "                    transforms.Grayscale(num_output_channels=1),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Grayscale(num_output_channels=1),\n",
    "                    transforms.Resize((32, 32)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "                ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if USE_ALBUMENTATIONS:\n",
    "            # Convert the normalized float image (from load_nodule_data) to standard 0-255 uint8.\n",
    "            img_rescaled = (img - np.min(img)) / (np.max(img) - np.min(img) + 1e-8)\n",
    "            img_uint8 = np.clip(img_rescaled * 255, 0, 255).astype(np.uint8)\n",
    "            # Albumentations expects image shape (H, W, C) even for grayscale.\n",
    "            img_uint8 = np.expand_dims(img_uint8, axis=-1)\n",
    "            transformed = self.transform(image=img_uint8)\n",
    "            img = transformed['image']\n",
    "        else:\n",
    "            img = np.expand_dims(img, 0).astype(np.uint8)\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            img = self.transform(img)\n",
    "        return img, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# -----------------------------\n",
    "# Advanced Capsule Network with Decoder (Reconstruction) Branch\n",
    "# -----------------------------\n",
    "class CapsNetWithDecoder(nn.Module):\n",
    "    def __init__(self, num_capsules, capsule_dim, num_classes, reconstruction_weight=0.0005, routing_iters=3):\n",
    "        super(CapsNetWithDecoder, self).__init__()\n",
    "        self.reconstruction_weight = reconstruction_weight\n",
    "        self.routing_iters = routing_iters\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 256, kernel_size=9, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout2d(0.3)\n",
    "        )\n",
    "        self.primary_capsules = nn.Sequential(\n",
    "            nn.Conv2d(256, num_capsules * capsule_dim, kernel_size=9, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(num_capsules * capsule_dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout2d(0.3)\n",
    "        )\n",
    "        self.num_capsules = num_capsules\n",
    "        self.capsule_dim = capsule_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(num_capsules * capsule_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(num_capsules * capsule_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 32 * 32),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def squash(self, x):\n",
    "        squared_norm = (x ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm + 1e-8)\n",
    "        return scale * x / (torch.sqrt(squared_norm) + 1e-8)\n",
    "\n",
    "    def dynamic_routing(self, u_hat):\n",
    "        batch_size, input_capsules, output_capsules, capsule_dim = u_hat.shape\n",
    "        b_ij = torch.zeros(batch_size, input_capsules, output_capsules, device=u_hat.device)\n",
    "        for iteration in range(self.routing_iters):\n",
    "            c_ij = F.softmax(b_ij, dim=2)\n",
    "            s_j = torch.sum(c_ij.unsqueeze(-1) * u_hat, dim=1)\n",
    "            v_j = self.squash(s_j)\n",
    "            if iteration < self.routing_iters - 1:\n",
    "                b_ij = b_ij + torch.sum(u_hat * v_j.unsqueeze(1), dim=-1)\n",
    "        return v_j\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        u_hat = self.primary_capsules(x)\n",
    "        batch_size = u_hat.size(0)\n",
    "        u_hat = u_hat.view(batch_size, self.num_capsules, self.capsule_dim, -1)\n",
    "        u_hat = u_hat.permute(0, 3, 1, 2)\n",
    "        v_j = self.dynamic_routing(u_hat)\n",
    "        v_j_flat = v_j.view(batch_size, -1)\n",
    "        class_output = self.classification_head(v_j_flat)\n",
    "        reconstruction = self.decoder(v_j_flat)\n",
    "        reconstruction = reconstruction.view(-1, 1, 32, 32)\n",
    "        return class_output, reconstruction\n",
    "\n",
    "# -----------------------------\n",
    "# Weighted Margin Loss Function\n",
    "# -----------------------------\n",
    "def margin_loss(y_true, y_pred, m_plus=0.9, m_minus=0.1, lambda_=0.5, class_weights=None):\n",
    "    \"\"\"\n",
    "    Advanced margin loss for capsule networks with optional class weights.\n",
    "    If class_weights is provided (a list or tensor of shape [num_classes]),\n",
    "    each sample's loss is multiplied by the weight corresponding to its true class.\n",
    "    \"\"\"\n",
    "    y_true_one_hot = F.one_hot(y_true, num_classes=y_pred.size(1)).float()\n",
    "    y_pred = torch.clamp(y_pred, min=0.0, max=1.0)\n",
    "    positive_loss = F.relu(m_plus - y_pred).pow(2) * y_true_one_hot\n",
    "    negative_loss = F.relu(y_pred - m_minus).pow(2) * (1 - y_true_one_hot)\n",
    "    loss = positive_loss + lambda_ * negative_loss\n",
    "    if class_weights is not None:\n",
    "        class_weights = torch.tensor(class_weights, device=y_true.device, dtype=torch.float32)\n",
    "        sample_weights = (y_true_one_hot * class_weights).sum(dim=1)\n",
    "        loss = loss.mean(dim=1) * sample_weights\n",
    "        return loss.mean()\n",
    "    return loss.mean()\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation Functions\n",
    "# -----------------------------\n",
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_targets, all_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, _ = model(data)\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            preds = output.argmax(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_targets, all_preds, average='binary'\n",
    "    )\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    fpr, tpr, _ = roc_curve(all_targets, [prob[1] for prob in all_probs])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'roc_auc': roc_auc,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'predictions': all_preds,\n",
    "        'targets': all_targets,\n",
    "        'probabilities': all_probs\n",
    "    }\n",
    "\n",
    "def visualize_training_metrics(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(132)\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(history['train_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(133)\n",
    "    plt.title('Learning Rate')\n",
    "    plt.plot(history['learning_rate'], label='Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_metrics.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def visualize_nodules(images, labels, num_samples=3):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(1, num_samples, i+1)\n",
    "        sample_idx = np.random.randint(len(images))\n",
    "        sample_img = images[sample_idx].squeeze()\n",
    "        label = labels[sample_idx]\n",
    "        plt.imshow(sample_img, cmap='gray')\n",
    "        plt.title(f'Nodule {\"Malignant\" if label == 1 else \"Benign\"}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('nodule_samples.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(cm):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Benign', 'Malignant'], \n",
    "                yticklabels=['Benign', 'Malignant'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, roc_auc):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig('roc_curve.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# -----------------------------\n",
    "# Main Training and Evaluation Loop with Early Stopping\n",
    "# -----------------------------\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    images, labels = load_nodule_data(max_scans=400)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        images, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    visualize_nodules(images, labels)\n",
    "    train_dataset = LungNoduleDataset(X_train, y_train, mode='train')\n",
    "    val_dataset = LungNoduleDataset(X_val, y_val, mode='val')\n",
    "\n",
    "    # Create a weighted sampler to balance classes if needed.\n",
    "    class_sample_count = np.array([np.sum(y_train == t) for t in np.unique(y_train)])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in y_train])\n",
    "    samples_weight = torch.from_numpy(samples_weight).float()\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = CapsNetWithDecoder(\n",
    "        num_capsules=12, \n",
    "        capsule_dim=16, \n",
    "        num_classes=2,\n",
    "        reconstruction_weight=0.0005,\n",
    "        routing_iters=3\n",
    "    ).to(device)\n",
    "    \n",
    "    # Use a slightly lower learning rate and add weight decay for regularization.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0008, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1)\n",
    "\n",
    "    # Compute class weights from training labels\n",
    "    classes, counts = np.unique(y_train, return_counts=True)\n",
    "    class_weights = {int(cls): float(1.0 / count) for cls, count in zip(classes, counts)}\n",
    "    print(\"Class weights:\", class_weights)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_accuracy': [],\n",
    "        'val_accuracy': [],\n",
    "        'learning_rate': []\n",
    "    }\n",
    "    num_epochs = 100\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    patience = 15\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0.0, 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            class_output, reconstruction = model(data)\n",
    "            # Pass class weights as a list in order of increasing class index.\n",
    "            loss_margin = margin_loss(target, class_output, class_weights=list(class_weights.values()))\n",
    "            loss_reconstruction = F.mse_loss(reconstruction, data)\n",
    "            loss = loss_margin + model.reconstruction_weight * loss_reconstruction\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            total_correct += (class_output.argmax(1) == target).sum().item()\n",
    "        train_loss = total_loss / len(train_dataset)\n",
    "        train_acc = total_correct / len(train_dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                class_output, reconstruction = model(data)\n",
    "                loss_margin = margin_loss(target, class_output, class_weights=list(class_weights.values()))\n",
    "                loss_reconstruction = F.mse_loss(reconstruction, data)\n",
    "                loss = loss_margin + model.reconstruction_weight * loss_reconstruction\n",
    "                val_loss += loss.item() * data.size(0)\n",
    "                val_correct += (class_output.argmax(1) == target).sum().item()\n",
    "        val_loss /= len(val_dataset)\n",
    "        val_acc = val_correct / len(val_dataset)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_accuracy'].append(train_acc)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | LR: {current_lr:.6f}\")\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "        scheduler.step(epoch + 1)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    visualize_training_metrics(history)\n",
    "    eval_results = evaluate_model(model, val_loader, device)\n",
    "    plot_confusion_matrix(eval_results['confusion_matrix'])\n",
    "    plot_roc_curve(eval_results['fpr'], eval_results['tpr'], eval_results['roc_auc'])\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        eval_results['targets'], \n",
    "        eval_results['predictions'], \n",
    "        target_names=['Benign', 'Malignant']\n",
    "    ))\n",
    "    torch.save(model.state_dict(), \"lung_capsnet_model_desktop_v1_improved.pth\")\n",
    "    print(\"Model saved as lung_capsnet_model_desktop_v1_improved.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
